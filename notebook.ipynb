{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dc467a-5830-44c0-ab74-435be0e5593c",
   "metadata": {},
   "source": [
    "Pneumonia is one of the leading respiratory illnesses worldwide, and its timely and accurate diagnosis is essential for effective treatment. Manually reviewing chest X-rays is a critical step in this process, and AI can provide valuable support by helping to expedite the assessment. Deep learning models can distinguish pneumonia cases from normal images of lungs in chest X-rays.\n",
    "\n",
    "By fine-tuning a pre-trained convolutional neural network, specifically the ResNet-50 model, you can classify X-ray images into two categories: normal lungs and those affected by pneumonia. You can leverage its already trained weights and get an accurate classifier trained faster and with fewer resources.\n",
    "\n",
    "## The Data\n",
    "\n",
    "<img src=\"x-rays_sample.png\" align=\"center\"/>\n",
    "&nbsp\n",
    "\n",
    "The dataset of chest X-rays have been preprocessed for use with a ResNet-50 model by calling `transforms.Resize(224)` and `transforms.CenterCrop(224)`. You can see a sample of 5 images from each category above. The dataset inside the `data/chestxrays` folder is divided into `test` and `train` folders.\n",
    "\n",
    "There are 150 training images and 50 testing images for each category, NORMAL and PNEUMONIA (300 and 100 in total). This data has been loaded into a `train_loader` and a `test_loader` using the `DataLoader` class from the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f522b79-2a5a-4472-adb9-0d924870bfa1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1727881526148,
    "lastExecutedByKernel": "7041c111-44bb-4574-8957-3984e7889c1e",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# # Make sure to run this cell to use torchmetrics.\n# !pip install torch torchvision torchmetrics",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# -------\n",
    "# Install\n",
    "# -------\n",
    "# !pip install torch torchvision torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1bedee-bcd5-4c80-a5ed-93df89af0295",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3927,
    "lastExecutedAt": 1727881370483,
    "lastExecutedByKernel": "7041c111-44bb-4574-8957-3984e7889c1e",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries\n# -------------------------\n# Data loading\nimport random\nimport numpy as np\nfrom torchvision.transforms import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\n# Train model\nimport torch\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Evaluate model\nfrom torchmetrics import Accuracy, F1Score\n\n# Set random seeds for reproducibility\ntorch.manual_seed(101010)\nnp.random.seed(101010)\nrandom.seed(101010)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Import required libraries\n",
    "# -------------------------\n",
    "\n",
    "# Data loading\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Train model\n",
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluate model\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "# Check for GPU availability\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# Check for MPS availability\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd91680d-cb63-4876-9a51-4ee6bb250c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Move 50 random images per class from the training set to create a validation set\n",
    "#---------------------------------------------------------------------------------\n",
    "def move_files(src_class_dir, dest_class_dir, n=50):\n",
    "    if not os.path.exists(dest_class_dir):\n",
    "        os.makedirs(dest_class_dir)\n",
    "    files = os.listdir(src_class_dir)\n",
    "    random_files = random.sample(files, n)\n",
    "    for f in random_files:\n",
    "        shutil.move(os.path.join(src_class_dir, f), os.path.join(dest_class_dir, f))\n",
    "\n",
    "if not os.path.exists('data/chestxrays/val'):\n",
    "    move_files('data/chestxrays/train/NORMAL', 'data/chestxrays/val/NORMAL')\n",
    "    move_files('data/chestxrays/train/PNEUMONIA', 'data/chestxrays/val/PNEUMONIA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc5591a-8dc1-4d7f-88d2-3b1a59fb2a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 200\n",
      "Validation set size: 100\n",
      "Test set size: 100\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------\n",
    "# Transformations and create datasets\n",
    "#------------------------------------\n",
    "\n",
    "# Define the transformations to apply to the images for use with ResNet-50.\n",
    "# The images need to be normalized to the same domain as the original training data of ResNet-50 network.\n",
    "# Normalize the X-rays using transforms. Normalize function that takes as input the means and\n",
    "# standard deviations of the three color channels, (R,G,B), from the original ResNet-50 training dataset.\n",
    "transform_mean = [0.485, 0.456, 0.406]\n",
    "transform_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transforms: Add horizontal flip for augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "])\n",
    "\n",
    "# Validation and test transforms: no augmentation, just normalization\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=transform_mean, std=transform_std)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageFolder('data/chestxrays/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('data/chestxrays/val', transform=val_test_transform)\n",
    "test_dataset = ImageFolder('data/chestxrays/test', transform=val_test_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "print(\"Validation set size:\", len(val_dataset))\n",
    "print(\"Test set size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99cf95b-83f3-49e4-9777-4e70736452d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenchettiath/Library/Mobile Documents/com~apple~CloudDocs/DataCamp/PyTorch/Projects/Classifying X-Ray Images using PyTorch/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6884, Train Acc: 0.5250, Val Loss: 0.6568, Val Acc: 0.5300\n",
      "Epoch [2/50], Train Loss: 0.5919, Train Acc: 0.8050, Val Loss: 0.6311, Val Acc: 0.8500\n",
      "Epoch [3/50], Train Loss: 0.5410, Train Acc: 0.8650, Val Loss: 0.6122, Val Acc: 0.8300\n",
      "Epoch [4/50], Train Loss: 0.4818, Train Acc: 0.9050, Val Loss: 0.5752, Val Acc: 0.7600\n",
      "Epoch [5/50], Train Loss: 0.4578, Train Acc: 0.8750, Val Loss: 0.5368, Val Acc: 0.8200\n",
      "Epoch [6/50], Train Loss: 0.3998, Train Acc: 0.9100, Val Loss: 0.5058, Val Acc: 0.8700\n",
      "Epoch [7/50], Train Loss: 0.3749, Train Acc: 0.9050, Val Loss: 0.4880, Val Acc: 0.9100\n",
      "Epoch [8/50], Train Loss: 0.3500, Train Acc: 0.9450, Val Loss: 0.4591, Val Acc: 0.8900\n",
      "Epoch [9/50], Train Loss: 0.3341, Train Acc: 0.9200, Val Loss: 0.4286, Val Acc: 0.9100\n",
      "Epoch [10/50], Train Loss: 0.3261, Train Acc: 0.9100, Val Loss: 0.4025, Val Acc: 0.9100\n",
      "Epoch [11/50], Train Loss: 0.3291, Train Acc: 0.9050, Val Loss: 0.3904, Val Acc: 0.9000\n",
      "Epoch [12/50], Train Loss: 0.3167, Train Acc: 0.9150, Val Loss: 0.3718, Val Acc: 0.9100\n",
      "Epoch [13/50], Train Loss: 0.2953, Train Acc: 0.9150, Val Loss: 0.3328, Val Acc: 0.9100\n",
      "Epoch [14/50], Train Loss: 0.2581, Train Acc: 0.9300, Val Loss: 0.3066, Val Acc: 0.9300\n",
      "Epoch [15/50], Train Loss: 0.2579, Train Acc: 0.9300, Val Loss: 0.2868, Val Acc: 0.9400\n",
      "Epoch [16/50], Train Loss: 0.2550, Train Acc: 0.9150, Val Loss: 0.2751, Val Acc: 0.9400\n",
      "Epoch [17/50], Train Loss: 0.2714, Train Acc: 0.9200, Val Loss: 0.2677, Val Acc: 0.9400\n",
      "Epoch [18/50], Train Loss: 0.2516, Train Acc: 0.9300, Val Loss: 0.2627, Val Acc: 0.9400\n",
      "Epoch [19/50], Train Loss: 0.2220, Train Acc: 0.9450, Val Loss: 0.2648, Val Acc: 0.9300\n",
      "Epoch [20/50], Train Loss: 0.2383, Train Acc: 0.9350, Val Loss: 0.2548, Val Acc: 0.9300\n",
      "Epoch [21/50], Train Loss: 0.2187, Train Acc: 0.9450, Val Loss: 0.2472, Val Acc: 0.9400\n",
      "Epoch [22/50], Train Loss: 0.2481, Train Acc: 0.9200, Val Loss: 0.2540, Val Acc: 0.9300\n",
      "Epoch [23/50], Train Loss: 0.2301, Train Acc: 0.9100, Val Loss: 0.2435, Val Acc: 0.9400\n",
      "Epoch [24/50], Train Loss: 0.1989, Train Acc: 0.9450, Val Loss: 0.2454, Val Acc: 0.9300\n",
      "Epoch [25/50], Train Loss: 0.1889, Train Acc: 0.9550, Val Loss: 0.2400, Val Acc: 0.9400\n",
      "Epoch [26/50], Train Loss: 0.1915, Train Acc: 0.9500, Val Loss: 0.2361, Val Acc: 0.9300\n",
      "Epoch [27/50], Train Loss: 0.1821, Train Acc: 0.9600, Val Loss: 0.2380, Val Acc: 0.9400\n",
      "Epoch [28/50], Train Loss: 0.1788, Train Acc: 0.9550, Val Loss: 0.2371, Val Acc: 0.9400\n",
      "Epoch [29/50], Train Loss: 0.1669, Train Acc: 0.9750, Val Loss: 0.2345, Val Acc: 0.9400\n",
      "Epoch [30/50], Train Loss: 0.1709, Train Acc: 0.9650, Val Loss: 0.2305, Val Acc: 0.9400\n",
      "Epoch [31/50], Train Loss: 0.1824, Train Acc: 0.9550, Val Loss: 0.2236, Val Acc: 0.9400\n",
      "Epoch [32/50], Train Loss: 0.1959, Train Acc: 0.9450, Val Loss: 0.2204, Val Acc: 0.9400\n",
      "Epoch [33/50], Train Loss: 0.1705, Train Acc: 0.9650, Val Loss: 0.2314, Val Acc: 0.9300\n",
      "Epoch [34/50], Train Loss: 0.1757, Train Acc: 0.9600, Val Loss: 0.2382, Val Acc: 0.9200\n",
      "Epoch [35/50], Train Loss: 0.1622, Train Acc: 0.9650, Val Loss: 0.2250, Val Acc: 0.9400\n",
      "Epoch [36/50], Train Loss: 0.1694, Train Acc: 0.9650, Val Loss: 0.2203, Val Acc: 0.9400\n",
      "Epoch [37/50], Train Loss: 0.1526, Train Acc: 0.9650, Val Loss: 0.2192, Val Acc: 0.9500\n",
      "Epoch [38/50], Train Loss: 0.1495, Train Acc: 0.9700, Val Loss: 0.2238, Val Acc: 0.9400\n",
      "Epoch [39/50], Train Loss: 0.1647, Train Acc: 0.9700, Val Loss: 0.2169, Val Acc: 0.9500\n",
      "Epoch [40/50], Train Loss: 0.1664, Train Acc: 0.9700, Val Loss: 0.2202, Val Acc: 0.9400\n",
      "Epoch [41/50], Train Loss: 0.1564, Train Acc: 0.9600, Val Loss: 0.2149, Val Acc: 0.9500\n",
      "Epoch [42/50], Train Loss: 0.1460, Train Acc: 0.9650, Val Loss: 0.2140, Val Acc: 0.9500\n",
      "Epoch [43/50], Train Loss: 0.1533, Train Acc: 0.9750, Val Loss: 0.2106, Val Acc: 0.9400\n",
      "Epoch [44/50], Train Loss: 0.1468, Train Acc: 0.9800, Val Loss: 0.2109, Val Acc: 0.9500\n",
      "Epoch [45/50], Train Loss: 0.1449, Train Acc: 0.9700, Val Loss: 0.2113, Val Acc: 0.9500\n",
      "Epoch [46/50], Train Loss: 0.1648, Train Acc: 0.9550, Val Loss: 0.2169, Val Acc: 0.9500\n",
      "Epoch [47/50], Train Loss: 0.1443, Train Acc: 0.9700, Val Loss: 0.2188, Val Acc: 0.9400\n",
      "Epoch [48/50], Train Loss: 0.1308, Train Acc: 0.9850, Val Loss: 0.2126, Val Acc: 0.9500\n",
      "Early stopping triggered\n",
      "Training complete. Best validation loss: 0.2106\n"
     ]
    }
   ],
   "source": [
    "#----------------------\n",
    "# Instantiate the model\n",
    "#----------------------\n",
    "\n",
    "# Load the pre-trained ResNet-50 model with ew weights with accuracy 80.858%\n",
    "resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "#-----------------\n",
    "# Modify the model\n",
    "#-----------------\n",
    "\n",
    "# Freeze the parameters of the model\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for binary classification\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, 1)\n",
    "\n",
    "# Set the model to ResNet-50\n",
    "model = resnet50\n",
    "\n",
    "# Move the model to the selected device (GPU, MPS, or CPU)\n",
    "model.to(device)\n",
    "\n",
    "#-------------------------\n",
    "# Define the training loop\n",
    "#-------------------------\n",
    "\n",
    "# Training function with validation and early stopping\n",
    "def train_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            \n",
    "            # Use mixed precision training for forward pass and loss computation\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        # train_acc = running_accuracy.double() / len(train_loader.dataset) # For GPU/CPU\n",
    "        train_acc = running_accuracy.float() / len(train_loader.dataset) # For MPS\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                labels = labels.float().unsqueeze(1)\n",
    "                \n",
    "                # Use mixed precision training for forward pass and loss computation\n",
    "                with torch.amp.autocast(device_type=device.type):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_accuracy += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        # val_acc = val_accuracy.double() / len(val_loader.dataset)  # For GPU/CPU\n",
    "        val_acc = val_accuracy.float() / len(val_loader.dataset) # For MPS\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete. Best validation loss: {:.4f}\".format(best_val_loss))\n",
    "\n",
    "#--------------------\n",
    "# Fine-tune the model\n",
    "#--------------------\n",
    "\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "# Use ReduceLROnPlateau scheduler to reduce LR if validation loss stagnates\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Decay lr by 10% every epoch (alternative scheduler)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Train the model with early stopping and validation\n",
    "train_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=50, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70761893-e66f-40fe-8862-dac9b18a13ab",
   "metadata": {},
   "source": [
    "### Below is the model evaluation code which evaluates the accuracy and F1-score of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0e1ad6-2f78-4a14-943b-8cc7c9dfe960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/rs1k98s56kg177yj9q6hybgc0000gn/T/ipykernel_25438/1022573388.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.810\n",
      "Test F1-score: 0.838\n"
     ]
    }
   ],
   "source": [
    "#-------------------\n",
    "# Evaluate the model\n",
    "#-------------------\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Initialize metrics for accuracy and F1 score\n",
    "accuracy_metric = Accuracy(task=\"binary\")\n",
    "f1_metric = F1Score(task=\"binary\")\n",
    "\n",
    "# Create lists store all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "  for inputs, labels in test_loader:\n",
    "    # Move inputs and labels to the device\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    preds = torch.sigmoid(outputs).round() # Round to 0 or 1\n",
    "    \n",
    "    # Extend the lists with predictions and labels\n",
    "    all_preds.extend(preds.cpu().tolist())\n",
    "    all_labels.extend(labels.unsqueeze(1).cpu().tolist())\n",
    "\n",
    "# Convert lists to tensors\n",
    "all_preds = torch.tensor(all_preds)\n",
    "all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# Compute metrics for the entire test set\n",
    "test_acc = accuracy_metric(all_preds, all_labels).item()\n",
    "test_f1 = f1_metric(all_preds, all_labels).item()\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test F1-score: {test_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
